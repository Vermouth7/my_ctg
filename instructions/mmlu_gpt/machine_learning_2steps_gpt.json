[
    {
        "instruction 1": "Evaluate the properties of linear regression estimators, specifically focusing on the Gauss-Markov theorem, which asserts that among the class of linear unbiased estimators, the linear regression estimator indeed has the smallest variance, and provide a proof or example to support this claim.",
        "instruction 2": "Analyze the characteristics of the AdaBoost algorithm, including how it assigns weights (coefficients) to classifiers. Confirm that these coefficients must be non-negative due to the nature of the boosting algorithm, which adjusts weights to improve performance on misclassified instances.",
        "question": "Statement 1| Linear regression estimator has the smallest variance among all unbiased estimators. Statement 2| The coefficients α assigned to the classifiers assembled by AdaBoost are always non-negative.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 3
    },
    {
        "instruction 1": "Analyze the claims made in both statements, identifying the key aspects of RoBERTa's training methodology compared to BERT, particularly focusing on the importance of the larger training corpus and its impact on performance.",
        "instruction 2": "Assess the accuracy of the statement regarding ResNeXt architectures and their use of activation functions, verifying whether they typically utilize tanh or if other activation functions, such as ReLU, are more prevalent in this context.",
        "question": "Statement 1| RoBERTa pretrains on a corpus that is approximate 10x larger than the corpus BERT pretrained on. Statement 2| ResNeXts in 2018 usually used tanh activation functions.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Analyze the differences between support vector machines (SVM) and logistic regression, specifically focusing on their underlying mechanisms for generating probability distributions over labels in classification tasks, and how these may influence the interpretation of model outputs.",
        "instruction 2": "Investigate how the changes in kernel functions from linear to higher order polynomial affect the support vectors in SVM, considering how the dimensionality of the data and the complexity of the decision boundary might alter the support vectors across different kernels.",
        "question": "Statement 1| Support vector machines, like logistic regression models, give a probability distribution over the possible labels given an input example. Statement 2| We would expect the support vectors to remain the same in general as we move from a linear kernel to higher order polynomial kernels.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Calculate the total number of different combinations of attribute values by multiplying the number of possible values for each attribute together, which in this case involves multiplying 3 (for the first attribute) by 2 (for the second attribute) by 2 (for the third attribute) by 2 (for the fourth attribute).",
        "instruction 2": "Multiply the resulting total combinations of attributes by the number of possible values for the class, which is 3, to find the maximum number of different examples that can be generated based on the given attributes and class.",
        "question": "A machine learning problem involves four attributes plus a class. The attributes have 3, 2, 2, and 2 possible values each. The class has 3 possible values. How many maximum possible different examples are there?",
        "choices": [
            "12",
            "24",
            "48",
            "72"
        ],
        "subject": "machine_learning",
        "answer": 3
    },
    {
        "instruction 1": "Identify and evaluate various deep learning architectures specifically designed for image classification, such as Convolutional Neural Networks (CNNs), ResNet, and Inception, focusing on their performance metrics for high-resolution images.",
        "instruction 2": "Consider advancements in architecture architectures up to 2020, including transfer learning techniques and enhancements made to existing networks that improve their classification accuracy on high-resolution datasets.",
        "question": "As of 2020, which architecture is best for classifying high-resolution images?",
        "choices": [
            "convolutional networks",
            "graph networks",
            "fully connected networks",
            "RBF networks"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Analyze Statement 1 to verify that the log-likelihood indeed increases with each iteration of the expectation-maximization (EM) algorithm, explaining the theoretical basis behind this property.",
        "instruction 2": "Evaluate Statement 2 by discussing the limitations of Q-learning, particularly focusing on the necessity of prior knowledge concerning the environment's dynamics, and distinguish this from other reinforcement learning approaches that can learn through exploration.",
        "question": "Statement 1| The log-likelihood of the data will always increase through successive iterations of the expectation maximation algorithm. Statement 2| One disadvantage of Q-learning is that it can only be used when the learner has prior knowledge of how its actions affect its environment.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Identify the mathematical representation of a gradient descent update, which typically involves subtracting a scaled version of the gradient vector g from the current parameter vector, considering the learning rate.",
        "instruction 2": "Evaluate the computational complexity of the gradient descent update process, which includes both the time taken to compute the scalar multiplication of the learning rate with the gradient and the vector subtraction operation, to determine the overall cost of a single update.",
        "question": "Let us say that we have computed the gradient of our cost function and stored it in a vector g. What is the cost of one gradient descent update given the gradient?",
        "choices": [
            "O(D)",
            "O(N)",
            "O(ND)",
            "O(ND^2)"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Analyze the properties of probability distribution functions, particularly for continuous random variables, to confirm that the probability density function p(x) must indeed satisfy the constraints of being non-negative and integrating to one over its domain.",
        "instruction 2": "Evaluate the concept of decision tree learning, focusing on the definition and calculation of information gain as it pertains to splitting nodes, ensuring to clarify the relationship between information gain and the construction of the decision tree.",
        "question": "Statement 1| For a continuous random variable x and its probability distribution function p(x), it holds that 0 ≤ p(x) ≤ 1 for all x. Statement 2| Decision tree is learned by minimizing information gain.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Analyze the structure of the Bayesian network to identify the nodes (H, U, P, W) and their relationships, determining the conditional dependencies and independencies among them.",
        "instruction 2": "Apply the formula for calculating the number of independent parameters needed in a Bayesian network, considering the number of nodes and their connections, specifically taking into account the configurations of parent-child relationships and any existing statistical dependencies.",
        "question": "Consider the Bayesian network given below. How many independent parameters are needed for this Bayesian Network H -> U <- P <- W?",
        "choices": [
            "2",
            "4",
            "8",
            "16"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Evaluate the concept of model convergence as the amount of training data increases, focusing on how the model's ability to generalize and improve its performance on unseen data relates to the size of the dataset.",
        "instruction 2": "Consider the implications of overfitting versus underfitting, analyzing how a model trained on an infinite number of examples might reach a point of optimal performance and accuracy, while potentially addressing diminishing returns beyond a certain point.",
        "question": "As the number of training examples goes to infinity, your model trained on that data will have:",
        "choices": [
            "Lower variance",
            "Higher variance",
            "Same variance",
            "None of the above"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Analyze the concept of VC-dimension and its relation to the ability of a hypothesis class, such as the set of rectangles in a 2D plane or the k-Nearest Neighbour classifier, to shatter a finite set of points, identifying the conditions under which shattering occurs.",
        "instruction 2": "Examine the characteristics of the k-Nearest Neighbour classifier, particularly with k = 1, to understand why it can classify any arrangement of points without error, implying that its VC-dimension is infinite, and how this contrasts with the behavior of rectangles in the 2D plane.",
        "question": "Statement 1| The set of all rectangles in the 2D plane (which includes non axisaligned rectangles) can shatter a set of 5 points. Statement 2| The VC-dimension of k-Nearest Neighbour classifier when k = 1 is infinite.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Define the term that describes a model with poor performance on both training and new data, emphasizing its characteristics such as overfitting or underfitting within the context of machine learning.",
        "instruction 2": "Examine contexts or examples in which models exhibit this behavior, clarifying the implications for machine learning applications and the importance of model selection and evaluation techniques.",
        "question": "_ refers to a model that can neither model the training data nor generalize to new data.",
        "choices": [
            "good fitting",
            "overfitting",
            "underfitting",
            "all of the above"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Analyze the definition and significance of the F1 score in evaluating classification models, particularly in scenarios with a high class imbalance, to understand its relevance and advantages compared to other metrics.",
        "instruction 2": "Examine the role of the ROC curve and the area under the ROC curve (AUC) in measuring the performance of anomaly detection systems, identifying how it provides insight into the trade-off between true positive rates and false positive rates in such contexts.",
        "question": "Statement 1| The F1 score can be especially useful for datasets with class high imbalance. Statement 2| The area under the ROC curve is one of the main metrics used to assess anomaly detectors.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Evaluate each statement independently to determine their validity in the context of neural networks and statistical learning theory, specifically assessing whether back-propagation guarantees a globally optimal solution and the implications of the VC dimension on model capacity.",
        "instruction 2": "Examine the relationship between the two statements by analyzing the concepts of optimization in neural networks and the geometric interpretation of the VC dimension, particularly how the capacity of a hypothesis class relates to the ability to shatter a given set of points.",
        "question": "Statement 1| The back-propagation algorithm learns a globally optimal neural network with hidden layers. Statement 2| The VC dimension of a line should be at most 2, since I can find at least one case of 3 points that cannot be shattered by any line.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Define high entropy in the context of information theory and understand how it relates to the distribution of classes in a classification problem, emphasizing that high entropy indicates a more uniform distribution among categories.",
        "instruction 2": "Analyze how the concept of partitions in classification can be influenced by entropy, clarifying that high entropy suggests less certainty in classification due to the mixed nature of the data partitions, leading to less clarity in distinguishing between classes.",
        "question": "High entropy means that the partitions in classification are",
        "choices": [
            "pure",
            "not pure",
            "useful",
            "useless"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Verify the accuracy of Statement 1 by examining the original ResNet paper, specifically looking for explicit mentions of Layer Normalization and Batch Normalization to determine which normalization technique was used.",
        "instruction 2": "Investigate the architecture and methodology of DCGANs to assess whether self-attention mechanisms are employed for training stabilization, and reference relevant literature that discusses advancements in GAN training techniques.",
        "question": "Statement 1| Layer Normalization is used in the original ResNet paper, not Batch Normalization. Statement 2| DCGANs use self-attention to stabilize training.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Analyze the implications of a high negative coefficient in a linear regression model, focusing on how it indicates an inverse relationship between that feature and the dependent variable, and what this means for predictions and interpretations.",
        "instruction 2": "Consider the context of the data set to understand why such a relationship might exist, reflecting on possible confounding factors or the nature of the feature itself that could explain the negative association.",
        "question": "In building a linear regression model for a particular data set, you observe the coefficient of one of the features having a relatively high negative value. This suggests that",
        "choices": [
            "This feature has a strong effect on the model (should be retained)",
            "This feature does not have a strong effect on the model (should be ignored)",
            "It is not possible to comment on the importance of this feature without additional information",
            "Nothing can be determined."
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Determine which structural assumption of a neural network, such as the number of layers, the number of neurons per layer, or the choice of activation functions, has the most significant impact on the model's capacity to generalize to unseen data, thereby influencing the balance between underfitting and overfitting.",
        "instruction 2": "Analyze how adjustments to this structural assumption can modify the model's complexity, and consider the implications of this complexity on the learning process, including the potential effects on training and validation performance.",
        "question": "For a neural network, which one of these structural assumptions is the one that most affects the trade-off between underfitting (i.e. a high bias model) and overfitting (i.e. a high variance model):",
        "choices": [
            "The number of hidden nodes",
            "The learning rate",
            "The initial choice of weights",
            "The use of a constant-term unit input"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Identify the structural assumptions underlying polynomial regression, focusing specifically on how the degree of the polynomial function impacts the model's ability to generalize from training data, which relates directly to the balance between underfitting and overfitting scenarios.",
        "instruction 2": "Examine the implications of model complexity in relation to the chosen polynomial degree, understanding that a higher degree may lead to overfitting while a lower degree can cause underfitting, and evaluate how this trade-off affects the overall model performance.",
        "question": "For polynomial regression, which one of these structural assumptions is the one that most affects the trade-off between underfitting and overfitting:",
        "choices": [
            "The polynomial degree",
            "Whether we learn the weights by matrix inversion or gradient descent",
            "The assumed variance of the Gaussian noise",
            "The use of a constant-term unit input"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Analyze the context of both statements to understand their implications regarding model accuracy and optimization in machine learning, focusing on how the accuracy mentioned in Statement 1 relates to the characteristics or configurations of models like ResNets in Statement 2.",
        "instruction 2": "Evaluate the significance of the optimization method (Adam optimizer) used in the training of ResNets, considering how this may affect the models' performance and the accuracy levels reported for different architectures on datasets like CIFAR-10.",
        "question": "Statement 1| As of 2020, some models attain greater than 98% accuracy on CIFAR-10. Statement 2| The original ResNets were not optimized with the Adam optimizer.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Define the K-means algorithm, focusing on its primary function of partitioning data into K clusters based on feature similarity, and clarify the iterative process involved in assigning data points to clusters and updating centroids.",
        "instruction 2": "Discuss the implications of selecting the number of clusters (K) and how it affects the outcome of the algorithm, as well as any common challenges faced, such as sensitivity to initialization and potential convergence to local minima.",
        "question": "The K-means algorithm:",
        "choices": [
            "Requires the dimension of the feature space to be no bigger than the number of samples",
            "Has the smallest value of the objective function when K = 1",
            "Minimizes the within class variance for a given number of clusters",
            "Converges to the global optimum if and only if the initial means are chosen as some of the samples themselves"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Analyze the characteristics of different neural network architectures, particularly focusing on VGGNets and AlexNet, to highlight the differences in kernel size and their implications for feature extraction.",
        "instruction 2": "Investigate the timeline of innovations in deep learning, specifically identifying the introduction and significance of data-dependent weight initialization in relation to batch normalization, to understand their respective roles in training deep neural networks.",
        "question": "Statement 1| VGGNets have convolutional kernels of smaller width and height than AlexNet's first-layer kernels. Statement 2| Data-dependent weight initialization procedures were introduced before Batch Normalization.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Determine the rank of the matrix by identifying the number of linearly independent rows or columns. This may involve performing row reduction (Gaussian elimination) to bring the matrix to its reduced row echelon form, assessing how many non-zero rows remain.",
        "instruction 2": "Consider the implications of matrix symmetry and linear dependence in this particular case, recognizing that all rows are identical in matrix A, which indicates that the rank is limited by the number of unique rows.",
        "question": "What is the rank of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]",
        "choices": [
            "0",
            "1",
            "2",
            "3"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Analyze the specifics of density estimation and its application in classification, particularly how kernel density estimators can be employed to estimate probability distributions for different classes, thereby aiding in the classification process.",
        "instruction 2": "Examine the mathematical relationship and assumptions underlying logistic regression and Gaussian Naive Bayes, focusing on how the equivalency of parameterization relates to their respective decision boundaries and classification capabilities.",
        "question": "Statement 1| Density estimation (using say, the kernel density estimator) can be used to perform classification. Statement 2| The correspondence between logistic regression and Gaussian Naive Bayes (with identity class covariances) means that there is a one-to-one correspondence between the parameters of the two classifiers.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Evaluate the characteristics of clustering methods that can accommodate spatial data, particularly focusing on their ability to form clusters of varying sizes and shapes. This involves considering techniques such as DBSCAN, which effectively identifies arbitrary-shaped clusters, compared to methods like K-means that assume spherical clusters.",
        "instruction 2": "Assess the specific requirements of the spatial data being analyzed, such as the density of the data points or the presence of noise, to determine which clustering method would be the most suitable for achieving the desired outcomes in terms of cluster diversity and shape.",
        "question": "Suppose we would like to perform clustering on spatial data such as the geometrical locations of houses. We wish to produce clusters of many different sizes and shapes. Which of the following methods is the most appropriate?",
        "choices": [
            "Decision Trees",
            "Density-based clustering",
            "Model-based clustering",
            "K-means clustering"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Analyze the mechanics of the AdaBoost algorithm by examining how the algorithm updates weights for misclassified examples and how this is related to the overall training error through iterations.",
        "instruction 2": "Evaluate the implications of the statements on the performance of weak classifiers in AdaBoost, particularly how multiplicative weight adjustments influence the learning process and contribute to the increase in weighted training error over iterations.",
        "question": "Statement 1| In AdaBoost weights of the misclassified examples go up by the same multiplicative factor. Statement 2| In AdaBoost, weighted training error e_t of the tth weak classifier on training data with weights D_t tends to increase as a function of t.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Identify the limitations and issues commonly associated with Maximum Likelihood Estimation (MLE), such as sensitivity to outliers, potential for overfitting, or bias in parameter estimates under certain conditions.",
        "instruction 2": "Examine situations where MLE might lead to misleading conclusions, especially in small sample sizes or when the model assumptions are violated, and consider alternative estimation methods that may provide more robust results.",
        "question": "MLE estimates are often undesirable because",
        "choices": [
            "they are biased",
            "they have high variance",
            "they are not consistent estimators",
            "None of the above"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Assess the computational steps involved in the gradient descent algorithm, including the calculation of gradients, updates to parameters, and the role of learning rates, to establish a clear understanding of the overall complexity.",
        "instruction 2": "Examine the influence of factors such as the dimensionality of the data, the convergence criteria, and optimization techniques (like batch vs. stochastic gradient descent) on the asymptotic behavior of the computation time related to gradient descent.",
        "question": "Computational complexity of Gradient descent is,",
        "choices": [
            "linear in D",
            "linear in N",
            "polynomial in D",
            "dependent on the number of iterations"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Explain the concept of ensemble learning and how averaging the output of multiple decision trees, such as in random forests, improves prediction stability and accuracy by reducing overfitting and variance.",
        "instruction 2": "Discuss the mechanism of combining the outputs from different decision trees and how this process leads to a more robust model compared to using a single decision tree, including the statistical advantages of averaging.",
        "question": "Averaging the output of multiple decision trees helps _.",
        "choices": [
            "Increase bias",
            "Decrease bias",
            "Increase variance",
            "Decrease variance"
        ],
        "subject": "machine_learning",
        "answer": 3
    },
    {
        "instruction 1": "Clarify the process of feature selection in linear regression, including how different subsets of features can lead to varying regression coefficients and model performance metrics, emphasizing the importance of the feature selection method applied.",
        "instruction 2": "Examine the significance of the training and validation stages in model development, noting how changes in the feature set during these stages can result in discrepancies in the final regression model outcomes.",
        "question": "The model obtained by applying linear regression on the identified subset of features may differ from the model obtained at the end of the process of identifying the subset during",
        "choices": [
            "Best-subset selection",
            "Forward stepwise selection",
            "Forward stage wise selection",
            "All of the above"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Define neural networks by explaining their fundamental components, such as neurons, layers, weights, and activation functions, while providing insight into how these elements work together to process and learn from data.",
        "instruction 2": "Discuss the various applications and types of neural networks (e.g., feedforward, convolutional, recurrent) to illustrate their versatility in solving different problems across fields such as computer vision, natural language processing, and more.",
        "question": "Neural networks:",
        "choices": [
            "Optimize a convex objective function",
            "Can only be trained with stochastic gradient descent",
            "Can use a mix of different activation functions",
            "None of the above"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Identify the total probability of testing positive by incorporating both scenarios: testing positive given the presence of the disease (true positives) and testing positive given the absence of the disease (false positives). Use the law of total probability by calculating P(TP) as P(TP|D)P(D) + P(TP|¬D)P(¬D), where ¬D represents not having the disease.",
        "instruction 2": "Calculate P(TP|¬D), which is the probability of testing positive when the patient does not have the disease. This is equal to 1 minus the probability of testing negative when not having the disease, which is 0.03. Use this value along with the provided probabilities to complete the total probability calculation for P(TP).",
        "question": "Say the incidence of a disease D is about 5 cases per 100 people (i.e., P(D) = 0.05). Let Boolean random variable D mean a patient “has disease D” and let Boolean random variable TP stand for \"tests positive.\" Tests for disease D are known to be very accurate in the sense that the probability of testing positive when you have the disease is 0.99, and the probability of testing negative when you do not have the disease is 0.97. What is P(TP), the prior probability of testing positive.",
        "choices": [
            "0.0368",
            "0.473",
            "0.078",
            "None of the above"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Analyze the implications of Statement 1 regarding the relationship between feature mapping using a radial basis kernel function and the performance of 1-NN classifiers in terms of classification accuracy, considering factors such as feature space complexity and data distribution.",
        "instruction 2": "Evaluate Statement 2 by comparing the VC dimension of a Perceptron with that of a simple linear Support Vector Machine (SVM), focusing on how each model's capacity to classify different patterns relates to their respective VC dimensions.",
        "question": "Statement 1| After mapped into feature space Q through a radial basis kernel function, 1-NN using unweighted Euclidean distance may be able to achieve better classification performance than in original space (though we can’t guarantee this). Statement 2| The VC dimension of a Perceptron is smaller than the VC dimension of a simple linear SVM.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Identify the key limitations of Grid Search in hyperparameter tuning for machine learning models, such as its computational inefficiency and the potential to miss optimal parameter combinations due to its exhaustive search method.",
        "instruction 2": "Explain how the dimensionality of the parameter space and the number of parameters can exacerbate these disadvantages, making Grid Search less feasible for complex models or larger datasets.",
        "question": "The disadvantage of Grid search is",
        "choices": [
            "It can not be applied to non-differentiable functions.",
            "It can not be applied to non-continuous functions.",
            "It is hard to implement.",
            "It runs reasonably slow for multiple linear regression."
        ],
        "subject": "machine_learning",
        "answer": 3
    },
    {
        "instruction 1": "Determine the nature of the problem in question by analyzing the factors involved in predicting rainfall, such as the reliance on environmental indicators, historical data, and statistical modeling to forecast weather patterns.",
        "instruction 2": "Categorize the problem type as one of uncertainty or estimation, considering how it might align with various fields like meteorology, data science, or environmental studies in its approach to prediction.",
        "question": "Predicting the amount of rainfall in a region based on various cues is a ______ problem.",
        "choices": [
            "Supervised learning",
            "Unsupervised learning",
            "Clustering",
            "None of the above"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Review each sentence provided regarding regression to determine their accuracy based on established statistical principles and theories related to regression analysis, including concepts such as correlation, causation, and regression coefficients.",
        "instruction 2": "Identify and differentiate supportive evidence or counterexamples that can be used to validate or disprove each statement, paying particular attention to common misconceptions in regression that could lead to a false assertion.",
        "question": "Which of the following sentence is FALSE regarding regression?",
        "choices": [
            "It relates inputs to outputs.",
            "It is used for prediction.",
            "It may be used for interpretation.",
            "It discovers causal relationships"
        ],
        "subject": "machine_learning",
        "answer": 3
    },
    {
        "instruction 1": "Identify and explain the concept of overfitting in decision trees, highlighting how excessive complexity can lead to poor generalization of the tree when applied to new data.",
        "instruction 2": "Discuss the techniques used for pruning decision trees, such as cost complexity pruning or reduced error pruning, and how these techniques contribute to improving model performance by simplifying the tree while maintaining accuracy.",
        "question": "Which one of the following is the main reason for pruning a Decision Tree?",
        "choices": [
            "To save computing time during testing",
            "To save space for storing the Decision Tree",
            "To make the training set error smaller",
            "To avoid overfitting the training set"
        ],
        "subject": "machine_learning",
        "answer": 3
    },
    {
        "instruction 1": "Evaluate the implications of Statement 1, focusing on the mathematical concept of kernel density estimation and how it relates to kernel regression, and ensure to understand the significance of the weighting applied to data points in these estimations.",
        "instruction 2": "Analyze Statement 2 by considering the properties of decision trees, particularly how the structure of a decision tree can grow in complexity regardless of the quantity of training data, leading to potential overfitting or high depth relative to the training examples.",
        "question": "Statement 1| The kernel density estimator is equivalent to performing kernel regression with the value Yi = 1/n at each point Xi in the original data set. Statement 2| The depth of a learned decision tree can be larger than the number of training examples used to create the tree.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Identify common techniques used to mitigate overfitting in machine learning models, such as regularization, cross-validation, pruning, and using simpler models, and evaluate each option provided to determine which one does not fit within these common strategies.",
        "instruction 2": "Assess the implications of each technique on model performance and complexity, ensuring to contextualize the option that is NOT valid as a means to reduce overfitting against established machine learning practices.",
        "question": "Suppose your model is overfitting. Which of the following is NOT a valid way to try and reduce the overfitting?",
        "choices": [
            "Increase the amount of training data.",
            "Improve the optimisation algorithm being used for error minimisation.",
            "Decrease the model complexity.",
            "Reduce the noise in the training data."
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Analyze Statement 1 to clarify the role of the softmax function in multiclass logistic regression, focusing on its purpose of converting logits to probabilities for multiple classes.",
        "instruction 2": "Examine Statement 2 to understand how temperature influences the distribution shape and entropy of the softmax function, considering how higher temperatures lead to a more uniform distribution and lower entropy, while lower temperatures create a more peaked distribution with higher entropy.",
        "question": "Statement 1| The softmax function is commonly used in mutliclass logistic regression. Statement 2| The temperature of a nonuniform softmax distribution affects its entropy.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Identify the fundamental principles of Support Vector Machines (SVM), such as the concept of maximizing the margin between different classes using support vectors, to establish a clear understanding of what makes SVMs distinct in classification tasks.",
        "instruction 2": "Evaluate the specific claims or statements provided about SVMs, critically analyzing each one against established theoretical knowledge and practical applications of SVM to determine their validity.",
        "question": "Which of the following is/are true regarding an SVM?",
        "choices": [
            "For two dimensional data points, the separating hyperplane learnt by a linear SVM will be a straight line.",
            "In theory, a Gaussian kernel SVM cannot model any complex separating hyperplane.",
            "For every kernel function used in a SVM, one can obtain an equivalent closed form basis expansion.",
            "Overfitting in an SVM is not a function of number of support vectors."
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Identify the structure of the Bayesian Network and the relationships between the variables H, U, P, and W, explicitly noting the directions of influence and dependency as presented in the diagram.",
        "instruction 2": "Set up the joint probability expression by applying the chain rule of probabilities, which states that the joint probability can be expressed as the product of the conditional probabilities, taking into account the given dependencies in the network.",
        "question": "Which of the following is the joint probability of H, U, P, and W described by the given Bayesian Network H -> U <- P <- W? [note: as the product of the conditional probabilities]",
        "choices": [
            "P(H, U, P, W) = P(H) * P(W) * P(P) * P(U)",
            "P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(W | H, P)",
            "P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)",
            "None of the above"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Evaluate the implications of the VC dimension concept and how it relates to the capacity and generalization ability of different SVM kernels, explaining the significance of having an infinite VC dimension in the context of model performance and complexity leading to overfitting.",
        "instruction 2": "Analyze the relationship between two-layer neural networks with linear activation functions and boosting algorithms that also use linear combinations, emphasizing the conditions under which these algorithms might converge to the same result or differ, thus clarifying the misconceptions in Statement 2.",
        "question": "Statement 1| Since the VC dimension for an SVM with a Radial Base Kernel is infinite, such an SVM must be worse than an SVM with polynomial kernel which has a finite VC dimension. Statement 2| A two layer neural network with linear activation functions is essentially a weighted combination of linear separators, trained on a given dataset; the boosting algorithm built on linear separators also finds a combination of linear separators, therefore these two algorithms will give the same result.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Analyze Statement 1 for its claims regarding the ID3 algorithm and its optimality, identifying any assumptions or conditions under which this statement holds true in the context of decision tree learning and information gain.",
        "instruction 2": "Examine Statement 2 to clarify the implications of a continuous probability distribution and how it relates to the concept of probability density functions, ensuring to highlight any mathematical principles that may support or contradict the assertion that the probability of a value x equals f(x).",
        "question": "Statement 1| The ID3 algorithm is guaranteed to find the optimal decision tree. Statement 2| Consider a continuous probability distribution with density f() that is nonzero everywhere. The probability of a value x is equal to f(x).",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Analyze the characteristics of the Neural Net described, focusing on the use of Entropy Loss and Sigmoid Activation Functions, to determine the appropriate optimization algorithms that are compatible with such configurations for finding the global optimum.",
        "instruction 2": "Research well-known optimization algorithms suited for training neural networks, such as Stochastic Gradient Descent, Adam, or RMSprop, considering their hyper-parameters and initialization techniques to ensure effectiveness in converging to the global optimum.",
        "question": "Given a Neural Net with N input nodes, no hidden layers, one output node, with Entropy Loss and Sigmoid Activation Functions, which of the following algorithms (with the proper hyper-parameters and initialization) can be used to find the global optimum?",
        "choices": [
            "Stochastic Gradient Descent",
            "Mini-Batch Gradient Descent",
            "Batch Gradient Descent",
            "All of the above"
        ],
        "subject": "machine_learning",
        "answer": 3
    },
    {
        "instruction 1": "Analyze how adding basis functions affects model complexity and the potential for overfitting in a linear model. Evaluate the trade-off between model flexibility and generalization based on the problem context.",
        "instruction 2": "Consider the criteria for selecting additional basis functions, such as their ability to capture nonlinear relationships or improve the model's predictive performance, and determine which option aligns best with these criteria.",
        "question": "Adding more basis functions in a linear model, pick the most probably option:",
        "choices": [
            "Decreases model bias",
            "Decreases estimation bias",
            "Decreases variance",
            "Doesn’t affect bias and variance"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Determine the number of nodes in the Bayesian network and the possible dependencies among them, calculating how many parameters are needed for each node based on the number of parents it has, considering that each edge represents a dependency and how this affects the independence assumptions.",
        "instruction 2": "Evaluate the full joint probability distribution required to describe the network, factoring in the lack of independence or conditional independence assumptions, to derive the total number of independent parameters needed for the Bayesian network.",
        "question": "Consider the Bayesian network given below. How many independent parameters would we need if we made no assumptions about independence or conditional independence H -> U <- P <- W?",
        "choices": [
            "3",
            "4",
            "7",
            "15"
        ],
        "subject": "machine_learning",
        "answer": 3
    },
    {
        "instruction 1": "Identify synonymous terms or phrases that relate to out-of-distribution detection in the context of machine learning and statistical classification, focusing on commonly used terminology in the literature.",
        "instruction 2": "Examine the nuances of different terms used in various contexts or fields, such as anomaly detection or novelty detection, to determine their relevance and interchangeability with out-of-distribution detection.",
        "question": "Another term for out-of-distribution detection is?",
        "choices": [
            "anomaly detection",
            "one-class detection",
            "train-test mismatch robustness",
            "background detection"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Analyze the implications of Statement 1 regarding the relationship between the classifier f and the weak learners h, particularly focusing on how boosting affects the parameters while maintaining the functional form of the decision boundary.",
        "instruction 2": "Evaluate the role of cross validation in boosting as presented in Statement 2, ensuring to outline how it contributes to preventing overfitting through the selection of the optimal number of iterations and assessing model performance on unseen data.",
        "question": "Statement 1| We learn a classifier f by boosting weak learners h. The functional form of f’s decision boundary is the same as h’s, but with different parameters. (e.g., if h was a linear classifier, then f is also a linear classifier). Statement 2| Cross validation can be used to select the number of iterations in boosting; this procedure may help reduce overfitting.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 3
    },
    {
        "instruction 1": "Analyze the architectural differences between highway networks, ResNets, and DenseNets, focusing on how each model processes information, specifically looking at the use of convolutions versus pooling layers and the implications for memory usage.",
        "instruction 2": "Evaluate the performance metrics and resource requirements of highway networks and DenseNets in comparison to ResNets, paying attention to factors such as model complexity, training efficiency, and memory consumption to contextualize the statements provided.",
        "question": "Statement 1| Highway networks were introduced after ResNets and eschew max pooling in favor of convolutions. Statement 2| DenseNets usually cost more memory than ResNets.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 3
    },
    {
        "instruction 1": "Clarify the relationship between the number of instances N in the training dataset and the classification runtime for nearest neighbors, focusing on how the increase in N influences computational complexity and time needed for classification.",
        "instruction 2": "Analyze the algorithmic steps involved in the nearest neighbor classification process, including distance calculations and how they scale with respect to N, to provide a clearer understanding of the runtime dependence on the size of the dataset.",
        "question": "If N is the number of instances in the training dataset, nearest neighbors has a classification run time of",
        "choices": [
            "O(1)",
            "O( N )",
            "O(log N )",
            "O( N^2 )"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Analyze the architectural characteristics of both ResNets and Transformers, emphasizing how their structural designs differ, particularly noting the role of self-attention mechanisms in Transformers and the skip connections in ResNets.",
        "instruction 2": "Clarify the distinctions in the operational principles of would-be 'feedforward' networks, focusing on how these mechanisms influence the strength and efficacy of learning processes in both architectures.",
        "question": "Statement 1| The original ResNets and Transformers are feedforward neural networks. Statement 2| The original Transformers use self-attention, but the original ResNet does not.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Analyze the properties of RELUs and sigmoids in the context of function behavior, specifically focusing on monotonicity, to understand how each activation function influences the learning dynamics of neural networks.",
        "instruction 2": "Evaluate the implications of gradient descent in training neural networks, considering the conditions under which convergence to the global optimum occurs, and how the choice of activation function may impact this convergence.",
        "question": "Statement 1| RELUs are not monotonic, but sigmoids are monotonic. Statement 2| Neural networks trained with gradient descent with high probability converge to the global optimum.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 3
    },
    {
        "instruction 1": "Identify the formula for the sigmoid function, which calculates the output of a sigmoid node, focusing on the mathematical expression of the function and its properties (e.g., range, asymptotic behavior).",
        "instruction 2": "Consider the input to the sigmoid node, typically a weighted sum of inputs plus a bias term, and analyze how varying these inputs affects the output, ensuring to understand how this relates to the neuron’s activation threshold.",
        "question": "The numerical output of a sigmoid node in a neural network:",
        "choices": [
            "Is unbounded, encompassing all real numbers.",
            "Is unbounded, encompassing all integers.",
            "Is bounded between 0 and 1.",
            "Is bounded between -1 and 1."
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Identify a specific machine learning model or algorithm that is specifically designed to work with linearly separable data, such as the perceptron or linear support vector machines (SVM), while contrasting it with models that can handle non-linear separability.",
        "instruction 2": "Analyze the characteristics of linearly separable data, emphasizing how a clear margin exists between different classes, and explain why this property is essential for the identified models to perform successfully.",
        "question": "Which of the following can only be used when training data are linearly separable?",
        "choices": [
            "Linear hard-margin SVM.",
            "Linear Logistic Regression.",
            "Linear Soft margin SVM.",
            "The centroid method."
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Identify and list commonly recognized spatial clustering algorithms, focusing on their characteristics and applications in handling spatial data, such as DBSCAN, K-Means, and Hierarchical Clustering.",
        "instruction 2": "Evaluate the differences between these algorithms, specifically in terms of how they define clusters, handle noise, and scale with data size, to provide a clear understanding of their respective strengths and weaknesses in spatial clustering contexts.",
        "question": "Which of the following are the spatial clustering algorithms?",
        "choices": [
            "Partitioning based clustering",
            "K-means clustering",
            "Grid based clustering",
            "All of the above"
        ],
        "subject": "machine_learning",
        "answer": 3
    },
    {
        "instruction 1": "Analyze the concepts present in both statements to identify the main arguments regarding support vector machines (SVMs) and their classification capabilities, focusing on the implications of maximum margin decision boundaries in relation to generalization error.",
        "instruction 2": "Evaluate the relationship between generative models and SVMs, specifically addressing how the polynomial kernel of degree three can represent decision boundaries and the limitations or conditions under which these representations hold true.",
        "question": "Statement 1| The maximum margin decision boundaries that support vector machines construct have the lowest generalization error among all linear classifiers. Statement 2| Any decision boundary that we get from a generative model with classconditional Gaussian distributions could in principle be reproduced with an SVM and a polynomial kernel of degree less than or equal to three.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 3
    },
    {
        "instruction 1": "Evaluate the properties of L1 and L2 regularization in linear models, specifically addressing how L2 regularization does not necessarily promote sparsity in the model coefficients compared to L1 regularization, which tends to produce sparse solutions by driving some coefficients to zero.",
        "instruction 2": "Examine the architectural characteristics of ResNets and Transformers with a focus on how residual connections work within these models, clarifying the role of such connections in facilitating deeper network training and enhancing model performance.",
        "question": "Statement 1| L2 regularization of linear models tends to make models more sparse than L1 regularization. Statement 2| Residual connections can be found in ResNets and Transformers.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 3
    },
    {
        "instruction 1": "Identify the variables involved in the calculation of P(H|E, F) and determine which joint or marginal probabilities are necessary to compute it without conditional independence information, ensuring to focus on the relationships among the variables H, E, and F.",
        "instruction 2": "Assess the data sets provided to identify which combinations of probabilities such as P(H), P(E), P(F), P(H, E), P(H, F), and P(E, F) can be used to derive P(H|E, F) using the formula P(H|E, F) = P(H, E, F) / P(E, F), without relying on conditional independence.",
        "question": "Suppose we like to calculate P(H|E, F) and we have no conditional independence information. Which of the following sets of numbers are sufficient for the calculation?",
        "choices": [
            "P(E, F), P(H), P(E|H), P(F|H)",
            "P(E, F), P(H), P(E, F|H)",
            "P(H), P(E|H), P(F|H)",
            "P(E, F), P(E|H), P(F|H)"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Analyze the concept of bagging (Bootstrap Aggregating) to understand how it reduces variance and helps in preventing overfitting by training multiple models on different subsets of the training data.",
        "instruction 2": "Examine specific mechanisms of bagging, such as the averaging of predictions from multiple models and the training on bootstrapped datasets, to highlight how these techniques contribute to improved generalization and less overfitting compared to a single model.",
        "question": "Which among the following prevents overfitting when we perform bagging?",
        "choices": [
            "The use of sampling with replacement as the sampling technique",
            "The use of weak classifiers",
            "The use of classification algorithms which are not prone to overfitting",
            "The practice of validation performed on every classifier trained"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Analyze the mathematical concepts and underlying principles of PCA and Spectral Clustering, focusing on how eigendecomposition is applied to the different matrices, ensuring clarity on the similarities and differences in their approaches despite the matrices being of the same size.",
        "instruction 2": "Explore the relationship between classification and regression tasks, providing a clear explanation of how logistic regression functions as a specialized form of linear regression, with particular emphasis on the conditions under which this classification occurs.",
        "question": "Statement 1| PCA and Spectral Clustering (such as Andrew Ng’s) perform eigendecomposition on two different matrices. However, the size of these two matrices are the same. Statement 2| Since classification is a special case of regression, logistic regression is a special case of linear regression.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Analyze the specific contents and primary uses of the Stanford Sentiment Treebank and the Penn Treebank to clarify the distinction between the two datasets, highlighting how one focuses on movie reviews while the other is utilized for language modeling.",
        "instruction 2": "Investigate the implications of the distinct data types in each dataset for natural language processing tasks, and consider how their unique characteristics influence the types of analyses and applications that can be derived from them.",
        "question": "Statement 1| The Stanford Sentiment Treebank contained movie reviews, not book reviews. Statement 2| The Penn Treebank has been used for language modeling.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Calculate the rank of the matrix A by performing row reduction to echelon form, and subsequently identify the number of linearly independent rows. This will inform you about the dimensionality of the column space.",
        "instruction 2": "Apply the rank-nullity theorem, which states that the dimension of the null space is equal to the number of columns minus the rank of the matrix, to find the dimensionality of the null space.",
        "question": "What is the dimensionality of the null space of the following matrix? A = [[3, 2, −9], [−6, −4, 18], [12, 8, −36]]",
        "choices": [
            "0",
            "1",
            "2",
            "3"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Define support vectors within the context of machine learning, specifically describing their role in the support vector machine (SVM) algorithm, which involves identifying the data points that are closest to the decision boundary and influence its position.",
        "instruction 2": "Explain how support vectors contribute to the overall model performance by impacting the margin between different classes, and discuss their significance in relation to overfitting and generalization in classification tasks.",
        "question": "What are support vectors?",
        "choices": [
            "The examples farthest from the decision boundary.",
            "The only examples necessary to compute f(x) in an SVM.",
            "The data centroid.",
            "All the examples that have a non-zero weight αk in a SVM."
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Analyze the implications of the two statements regarding Word2Vec and the Restricted Boltzmann Machine, focusing on how the initialization of parameters influences model performance and the relationship between these two concepts.",
        "instruction 2": "Examine the significance of the tanh function as a nonlinear activation function in neural networks, considering how its properties affect the behavior of models, particularly in terms of learning capabilities and output generation.",
        "question": "Statement 1| Word2Vec parameters were not initialized using a Restricted Boltzman Machine. Statement 2| The tanh function is a nonlinear activation function.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Analyze potential issues related to model training, such as overfitting, where the model begins to memorize the training data rather than learning general patterns, which may lead to an increase in training loss over epochs.",
        "instruction 2": "Investigate other possible causes for increasing training loss, including but not limited to inappropriate learning rates, data quality issues, or model complexity that may not align with the given task.",
        "question": "If your training loss increases with number of epochs, which of the following could be a possible issue with the learning process?",
        "choices": [
            "Regularization is too low and model is overfitting",
            "Regularization is too high and model is underfitting",
            "Step size is too large",
            "Step size is too small"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Apply Bayes' theorem to find P(D | TP), which involves calculating P(TP | D), P(D), and P(TP). This includes determining the probability of a positive test given the disease is present and the overall probabilities of disease and test results.",
        "instruction 2": "Ensure to calculate P(TP) as the total probability of testing positive, which comprises the probability of true positives (when the disease is present) and false positives (when the disease is not present), factoring in the incidence of the disease and test accuracy.",
        "question": "Say the incidence of a disease D is about 5 cases per 100 people (i.e., P(D) = 0.05). Let Boolean random variable D mean a patient “has disease D” and let Boolean random variable TP stand for \"tests positive.\" Tests for disease D are known to be very accurate in the sense that the probability of testing positive when you have the disease is 0.99, and the probability of testing negative when you do not have the disease is 0.97. What is P(D | TP), the posterior probability that you have disease D when the test is positive?",
        "choices": [
            "0.0495",
            "0.078",
            "0.635",
            "0.97"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Analyze the implications of Statement 1 regarding the assumptions in traditional machine learning, particularly how this affects model performance and generalization when the train and test sets are not independent and identically distributed.",
        "instruction 2": "Evaluate Statement 2 by examining the significance of pretraining models on ImageNet for COCO in 2017, exploring how this practice influences transfer learning and the effectiveness of models in task-specific applications.",
        "question": "Statement 1| Traditional machine learning results assume that the train and test sets are independent and identically distributed. Statement 2| In 2017, COCO models were usually pretrained on ImageNet.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Analyze the implications of Statement 1 regarding the performance of classifiers and the relationship between training margins and test set efficacy, ensuring to clarify that the margins do not directly correlate with test performance without considering other factors such as data distribution and model complexity.",
        "instruction 2": "Evaluate the significance of Statement 2 in the context of BERT's architecture, recognizing that the GELU activation function contributes to the model's performance and behavior, and correlating it with general neural network principles regarding activation functions.",
        "question": "Statement 1| The values of the margins obtained by two different kernels K1(x, x0) and K2(x, x0) on the same training set do not tell us which classifier will perform better on the test set. Statement 2| The activation function of BERT is the GELU.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Review a list of common machine learning algorithms and categorize them based on their functionality, specifically focusing on algorithms designed for clustering, such as K-Means, Hierarchical Clustering, and DBSCAN.",
        "instruction 2": "Clearly differentiate clustering algorithms from other types of algorithms in machine learning, such as classification and regression algorithms, by understanding their specific objectives and methods of operation.",
        "question": "Which of the following is a clustering algorithm in machine learning?",
        "choices": [
            "Expectation Maximization",
            "CART",
            "Gaussian Naïve Bayes",
            "Apriori"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Evaluate potential issues related to the decision tree model's complexity, such as overfitting or underfitting, considering whether the tree depth is appropriate for the training data size and if the model is too simplistic or too complex.",
        "instruction 2": "Assess the quality and relevance of the input features used in the training data, analyzing if they effectively capture the distinctions between spam and non-spam messages, including checking for data imbalance or noise that could be impacting model performance.",
        "question": "You've just finished training a decision tree for spam classification, and it is getting abnormally bad performance on both your training and test sets. You know that your implementation has no bugs, so what could be causing the problem?",
        "choices": [
            "Your decision trees are too shallow.",
            "You need to increase the learning rate.",
            "You are overfitting.",
            "None of the above."
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Define K-fold cross-validation by explaining the process of dividing a dataset into K equally sized folds, where each fold is used once as a validation set while the remaining K-1 folds are used for training, to assess the model's performance.",
        "instruction 2": "Discuss the advantages of K-fold cross-validation, such as reducing overfitting and providing a more reliable estimate of model performance compared to a single train-test split, while also considering how the choice of K affects the variance and bias of the evaluation.",
        "question": "K-fold cross-validation is",
        "choices": [
            "linear in K",
            "quadratic in K",
            "cubic in K",
            "exponential in K"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Evaluate the accuracy of Statement 1 regarding the training of neural networks on CPUs versus GPUs, considering the current trends and common practices in the field of machine learning.",
        "instruction 2": "Analyze the implications of Statement 2 about the ResNet-50 model having over 1 billion parameters, focusing on how this relates to the computational requirements and efficiency of training processes involving large neural networks.",
        "question": "Statement 1| Industrial-scale neural networks are normally trained on CPUs, not GPUs. Statement 2| The ResNet-50 model has over 1 billion parameters.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Use the law of total probability to express P(A) in terms of P(A | B) and P(A | ¬B), taking into account the probabilities of B and ¬B. This involves setting up the equation P(A) = P(A | B)P(B) + P(A | ¬B)P(¬B).",
        "instruction 2": "Substituting the values provided, calculate P(A | B) by rearranging the equation from the first instruction, ensuring to use values for P(B), P(¬B) (which is 1 - P(B)), and P(A | ¬B) in the computation.",
        "question": "Given two Boolean random variables, A and B, where P(A) = 1/2, P(B) = 1/3, and P(A | ¬B) = 1/4, what is P(A | B)?",
        "choices": [
            "1/6",
            "1/4",
            "3/4",
            "1"
        ],
        "subject": "machine_learning",
        "answer": 3
    },
    {
        "instruction 1": "Identify the prominent scholars and professors who have extensively researched and discussed existential risks associated with artificial intelligence, focusing on their contributions and perspectives within the field.",
        "instruction 2": "Determine the specific theories or ideas proposed by these professors that link their work to the broader discourse on AI risks, parsing out any influential papers or public statements that have shaped the understanding of such existential threats.",
        "question": "Existential risks posed by AI are most commonly associated with which of the following professors?",
        "choices": [
            "Nando de Frietas",
            "Yann LeCun",
            "Stuart Russell",
            "Jitendra Malik"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Analyze the implications of each statement about statistical models and classifiers, focusing on the concepts of local optima in logistic regression and the established performance benchmarks of the naive Bayes classifier under known distributions.",
        "instruction 2": "Examine the contexts in which these statements apply, considering the conditions under which logistic regression might yield multiple local optima and the specific scenarios where naïve Bayes may be superior or inferior compared to other classifiers.",
        "question": "Statement 1| Maximizing the likelihood of logistic regression model yields multiple local optimums. Statement 2| No classifier can do better than a naive Bayes classifier if the distribution of the data is known.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Identify the specific structural assumptions associated with Kernel Regression, such as the choice of bandwidth, the kernel function, or the assumption of smoothness in the underlying function, and critically analyze how each of these impacts the model's bias and variance trade-off.",
        "instruction 2": "Evaluate how varying the bandwidth selection in Kernel Regression influences the model's flexibility, leading to underfitting when the bandwidth is too large and overfitting when it is too small, thus determining which structural assumption has the most significant effect on this trade-off.",
        "question": "For Kernel Regression, which one of these structural assumptions is the one that most affects the trade-off between underfitting and overfitting:",
        "choices": [
            "Whether kernel function is Gaussian versus triangular versus box-shaped",
            "Whether we use Euclidian versus L1 versus L∞ metrics",
            "The kernel width",
            "The maximum height of the kernel function"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Analyze the claims made in both statements regarding machine learning algorithms, focusing on the implications of guarantees in optimality for SVM and the potential for improved performance of Perceptron when a kernel function is applied.",
        "instruction 2": "Evaluate the differences in the theoretical foundations of SVM and Perceptron models, specifically addressing the significance of convex optimization in SVM and the non-convex nature of Perceptron learning in its original space.",
        "question": "Statement 1| The SVM learning algorithm is guaranteed to find the globally optimal hypothesis with respect to its object function. Statement 2| After being mapped into feature space Q through a radial basis kernel function, a Perceptron may be able to achieve better classification performance than in its original space (though we can’t guarantee this).",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Identify the key structural assumptions of a Gaussian Bayes classifier, such as the assumption of normal distribution of the features within each class, and analyze how each of these assumptions impacts model complexity and bias-variance trade-off.",
        "instruction 2": "Evaluate the relationship between the assumption of feature independence and the risk of overfitting versus underfitting, considering how deviations from this assumption may lead to poor performance on unseen data and influence the model's generalization ability.",
        "question": "For a Gaussian Bayes classifier, which one of these structural assumptions is the one that most affects the trade-off between underfitting and overfitting:",
        "choices": [
            "Whether we learn the class centers by Maximum Likelihood or Gradient Descent",
            "Whether we assume full class covariance matrices or diagonal class covariance matrices",
            "Whether we have equal class priors or priors estimated from the data",
            "Whether we allow classes to have different mean vectors or we force them to share the same mean vector"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Analyze both statements critically to identify the definitions and implications of overfitting in the context of machine learning, focusing on how the size of training data and the hypothesis space relate to model generalization.",
        "instruction 2": "Evaluate the relationship between model complexity and data availability, considering how a small training dataset may lead to overfitting regardless of the size of the hypothesis space, and conversely, how a small hypothesis space may restrict the model's ability to learn from a larger dataset.",
        "question": "Statement 1| Overfitting is more likely when the set of training data is small. Statement 2| Overfitting is more likely when the hypothesis space is small.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 3
    },
    {
        "instruction 1": "Analyze the context of the two statements to clarify how gradient descent can be applied to Gaussian mixture models in comparison to other methods like Expectation-Maximization (EM), ensuring to define terms accurately.",
        "instruction 2": "Evaluate the implications of the second statement regarding the learning time of a Gaussian-based Bayes optimal classifier, considering its relationship to the size of the dataset and the computational complexity involved in the learning process.",
        "question": "Statement 1| Besides EM, gradient descent can be used to perform inference or learning on Gaussian mixture model. Statement 2 | Assuming a fixed number of attributes, a Gaussian-based Bayes optimal classifier can be learned in time linear in the number of records in the dataset.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Analyze the properties of Bayesian networks to determine the conditions under which the junction tree algorithm and variable elimination yield equivalent inference results, paying special attention to the graphical structure and the dependencies between variables.",
        "instruction 2": "Explore the concept of d-separation within Bayesian networks, particularly focusing on how conditional independence between two random variables given a third influences the network's structure and interpretations, and verify the implication of this d-separation on the graphical representation of the nodes.",
        "question": "Statement 1| In a Bayesian network, the inference results of the junction tree algorithm are the same as the inference results of variable elimination. Statement 2| If two random variable X and Y are conditionally independent given another random variable Z, then in the corresponding Bayesian network, the nodes for X and Y are d-separated given Z.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Determine the nature of the learning problem by identifying the characteristics of the data provided (medical records) and the objective of discovering different patient clusters, which suggests the use of unsupervised learning techniques such as clustering algorithms.",
        "instruction 2": "Consider the implications of clustering analysis in healthcare, including how it can lead to personalized treatment solutions and the importance of evaluating the resulting clusters for clinical relevance and actionable insights.",
        "question": "Given a large dataset of medical records from patients suffering from heart disease, try to learn whether there might be different clusters of such patients for which we might tailor separate treatments. What kind of learning problem is this?",
        "choices": [
            "Supervised learning",
            "Unsupervised learning",
            "Both (a) and (b)",
            "Neither (a) nor (b)"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Identify the mathematical relationship between Principal Component Analysis (PCA) and Singular Value Decomposition (SVD), emphasizing how PCA can be computed by utilizing the SVD of the data matrix.",
        "instruction 2": "Outline the steps required to perform PCA using SVD, including centering the data, applying SVD to decompose the covariance matrix, and interpreting the singular vectors as the principal components.",
        "question": "What would you do in PCA to get the same projection as SVD?",
        "choices": [
            "Transform data to zero mean",
            "Transform data to zero median",
            "Not possible",
            "None of these"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Analyze the implications of both statements separately to determine their validity in the context of statistical learning and Bayesian inference, noting the specific conditions under which each statement holds true.",
        "instruction 2": "Evaluate how the consistency of the 1-nearest neighbor classifier's training error relates to the convergence properties of Bayesian estimators as data increases, particularly focusing on how the choice of prior may influence the MAP estimate in practical scenarios.",
        "question": "Statement 1| The training error of 1-nearest neighbor classifier is 0. Statement 2| As the number of data points grows to infinity, the MAP estimate approaches the MLE estimate for all possible priors. In other words, given enough data, the choice of prior is irrelevant.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Analyze the relationship between the regularisation parameter λ and its impact on model complexity, particularly how increasing λ penalises the coefficients of the model, potentially leading to higher bias and lower variance, which in turn affects the testing error.",
        "instruction 2": "Examine the trade-off inherent in least-squares regression with regularisation, considering how increased regularisation can help prevent overfitting but may also reduce the model's ability to capture the underlying data distribution, thereby influencing the testing error in the context of the data provided.",
        "question": "When doing least-squares regression with regularisation (assuming that the optimisation can be done exactly), increasing the value of the regularisation parameter λ the testing error.",
        "choices": [
            "will never decrease the training error.",
            "will never increase the training error.",
            "will never decrease the testing error.",
            "will never increase"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Examine the fundamental objective of discriminative approaches in machine learning, focusing on how they differentiate between classes by modeling the decision boundary directly rather than modeling the data distribution of each class.",
        "instruction 2": "Clarify how the parameters (w) in the model relate to the features used in the classification task, emphasizing the role these parameters play in shaping the decision boundary between different classes.",
        "question": "Which of the following best describes what discriminative approaches try to model? (w are the parameters in the model)",
        "choices": [
            "p(y|x, w)",
            "p(y, x)",
            "p(w|x, w)",
            "None of the above"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Analyze the statements to identify the relationship between the performance of convolutional neural networks on the CIFAR-10 dataset and the efficacy of ensemble methods in improving classification accuracy, noting the potential contradictions or confirmations between the two statements.",
        "instruction 2": "Evaluate empirical evidence or research findings that either support or contradict the claims made in each statement, focusing on how correlation among representations in ensembles can impact their collective performance on tasks like CIFAR-10 classification.",
        "question": "Statement 1| CIFAR-10 classification performance for convolution neural networks can exceed 95%. Statement 2| Ensembles of neural networks do not improve classification accuracy since the representations they learn are highly correlated.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Identify the fundamental philosophical differences between Bayesian and frequentist statistics, particularly focusing on their interpretations of probability, such as whether probability represents a degree of belief (Bayesian) or long-run frequencies of events (frequentist).",
        "instruction 2": "Analyze specific statistical methods or scenarios where these differing philosophies lead to divergent conclusions or opinions, such as the treatment of prior information in Bayesian analysis versus its exclusion in frequentist approaches.",
        "question": "Which of the following points would Bayesians and frequentists disagree on?",
        "choices": [
            "The use of a non-Gaussian noise model in probabilistic regression.",
            "The use of probabilistic modelling for regression.",
            "The use of prior distributions on the parameters in a probabilistic model.",
            "The use of class priors in Gaussian Discriminant Analysis."
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Analyze the definitions and methodologies of the BLEU and ROGUE metrics, emphasizing the metrics' focus on precision and recall respectively, and assess how these concepts relate to language processing and evaluation.",
        "instruction 2": "Investigate the role of Hidden Markov Models (HMM) in natural language processing, specifically their application in modeling grammatical structures in English sentences, and contrast this with the use of other statistical or machine learning models in the field.",
        "question": "Statement 1| The BLEU metric uses precision, while the ROGUE metric uses recall. Statement 2| Hidden markov models were frequently used to model English sentences.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Analyze the comparative aspects of the two statements focused on the quantity and quality of images in the datasets, specifically addressing the resolution variations in ImageNet and the general image count in Caltech-101.",
        "instruction 2": "Investigate any specific data or studies that detail the characteristics of both ImageNet and Caltech-101 to determine the validity of the statements presented, particularly examining the significance of resolution in relation to the total image count.",
        "question": "Statement 1| ImageNet has images of various resolutions. Statement 2| Caltech-101 has more images than ImageNet.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Identify the various methods of feature selection and their suitability based on the context of the problem, considering aspects such as model interpretability, the nature of the data, and the objectives of the analysis.",
        "instruction 2": "Evaluate the merits and limitations of each feature selection technique in relation to the specific dataset and analysis goals, ensuring to justify the choice of the most appropriate method based on performance or relevance to the problem.",
        "question": "Which of the following is more appropriate to do feature selection?",
        "choices": [
            "Ridge",
            "Lasso",
            "both (a) and (b)",
            "neither (a) nor (b)"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Identify the key steps in the EM algorithm that involve the estimation of parameters, distinguishing between likelihood-based estimates and those based on posterior probabilities, which are essential for calculating MAP estimates.",
        "instruction 2": "Analyze the E-step where the expected value of the complete data log-likelihood is computed, and adjust it to incorporate prior distributions on the parameters, ensuring that the modified algorithm reflects the influence of these priors in the parameter estimation process.",
        "question": "Suppose you are given an EM algorithm that finds maximum likelihood estimates for a model with latent variables. You are asked to modify the algorithm so that it finds MAP estimates instead. Which step or steps do you need to modify?",
        "choices": [
            "Expectation",
            "Maximization",
            "No modification necessary",
            "Both"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Identify the key structural assumptions of a Gaussian Bayes classifier, such as the assumption of normal distribution of the features within each class, and analyze how each of these assumptions impacts model complexity and bias-variance trade-off.",
        "instruction 2": "Evaluate the relationship between the assumption of feature independence and the risk of overfitting versus underfitting, considering how deviations from this assumption may lead to poor performance on unseen data and influence the model's generalization ability.",
        "question": "For a Gaussian Bayes classifier, which one of these structural assumptions is the one that most affects the trade-off between underfitting and overfitting:",
        "choices": [
            "Whether we learn the class centers by Maximum Likelihood or Gradient Descent",
            "Whether we assume full class covariance matrices or diagonal class covariance matrices",
            "Whether we have equal class priors or priors estimated from the data",
            "Whether we allow classes to have different mean vectors or we force them to share the same mean vector"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Analyze the principles of entropy and the concept of joint distributions to clearly understand the relationship stated in Statement 1, focusing on the implications of joint entropy being greater than the sum of individual entropies.",
        "instruction 2": "Examine the process of moralization in directed graphs to explain how it can lead to a reduction in the number of edges, considering the structural changes that occur during moralization and their effects on the graph's properties.",
        "question": "Statement 1| For any two variables x and y having joint distribution p(x, y), we always have H[x, y] ≥ H[x] + H[y] where H is entropy function. Statement 2| For some directed graphs, moralization decreases the number of edges present in the graph.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Analyze the provided options to identify characteristics of supervised learning, focusing on how supervised learning uses labeled data for training models, and determine which option lacks this feature.",
        "instruction 2": "Evaluate the definitions and concepts associated with unsupervised learning, semi-supervised learning, and reinforcement learning to ensure a clear distinction from supervised learning in identifying the correct answer.",
        "question": "Which of the following is NOT supervised learning?",
        "choices": [
            "PCA",
            "Decision Tree",
            "Linear Regression",
            "Naive Bayesian"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Analyze the role of learning rate in neural network training, particularly how it affects convergence speed and stability, and clarify why an inappropriate learning rate can lead to poor outcomes.",
        "instruction 2": "Examine the concept of dropout in neural networks, explaining its purpose in preventing overfitting by randomly setting a proportion of neuron activations to zero, and how this can influence the overall model performance.",
        "question": "Statement 1| A neural network's convergence depends on the learning rate. Statement 2| Dropout multiplies randomly chosen activation values by zero.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Identify the possible logical expressions or rules governing the relationship between the Boolean random variables A, B, and C in the absence of independence assumptions, such as examining the joint distribution and how it can be represented using logical operations.",
        "instruction 2": "Evaluate common formulas or identities in Boolean algebra, such as the inclusion-exclusion principle, to rewrite or express P(A, B, C) based on the relationships and dependencies among A, B, and C.",
        "question": "Which one of the following is equal to P(A, B, C) given Boolean random variables A, B and C, and no independence or conditional independence assumptions between any of them?",
        "choices": [
            "P(A | B) * P(B | C) * P(C | A)",
            "P(C | A, B) * P(A) * P(B)",
            "P(A, B | C) * P(C)",
            "P(A | B, C) * P(B | A, C) * P(C | A, B)"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Identify the core principles of clustering as a data analysis technique, focusing on its ability to group similar data points based on specified features without prior knowledge of categories.",
        "instruction 2": "Evaluate the specific nature of the tasks listed, determining which ones involve grouping or segmenting data into distinct clusters based on inherent characteristics, rather than classification or regression tasks.",
        "question": "Which of the following tasks can be best solved using Clustering.",
        "choices": [
            "Predicting the amount of rainfall based on various cues",
            "Detecting fraudulent credit card transactions",
            "Training a robot to solve a maze",
            "All of the above"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Identify the types of regularization techniques used in linear regression, specifically distinguishing between L1 (Lasso) and L2 (Ridge) penalties, and determine how each affects the coefficients of the model.",
        "instruction 2": "Analyze the implications of zeroed coefficients in the context of model simplification and feature selection, particularly focusing on how L1 regularization encourages sparsity in the coefficient estimates.",
        "question": "After applying a regularization penalty in linear regression, you find that some of the coefficients of w are zeroed out. Which of the following penalties might have been used?",
        "choices": [
            "L0 norm",
            "L1 norm",
            "L2 norm",
            "either (a) or (b)"
        ],
        "subject": "machine_learning",
        "answer": 3
    },
    {
        "instruction 1": "Analyze the relationship between the probabilities of the events A and B, specifically focusing on the implications of P(A, B) decreasing while P(A) increases. This may involve considering conditional probabilities and potential independence between the events.",
        "instruction 2": "Evaluate various scenarios or interpretations of the events A and B to determine what inferences can be drawn about the relationship between them, such as whether they are complementary events or if one event's occurrence influences the likelihood of the other.",
        "question": "A and B are two events. If P(A, B) decreases while P(A) increases, which of the following is true?",
        "choices": [
            "P(A|B) decreases",
            "P(B|A) decreases",
            "P(B) decreases",
            "All of above"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Analyze the two statements separately to identify key concepts in Hidden Markov Models (HMM) and collaborative filtering, focusing on the implications of allowing more hidden states in HMMs and how collaborative filtering applies to user preferences.",
        "instruction 2": "Evaluate the relationship between the statements, particularly how the flexibility in modeling (increasing hidden states in HMMs) relates to the effectiveness of collaborative filtering in capturing user preferences, to form a coherent understanding of both concepts.",
        "question": "Statement 1| When learning an HMM for a fixed set of observations, assume we do not know the true number of hidden states (which is often the case), we can always increase the training data likelihood by permitting more hidden states. Statement 2| Collaborative filtering is often a useful model for modeling users' movie preference.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "question": "You are training a linear regression model for a simple estimation task, and notice that the model is overfitting to the data. You decide to add in $\\ell_2$ regularization to penalize the weights. As you increase the $\\ell_2$ regularization coefficient, what will happen to the bias and variance of the model?",
        "subject": "machine_learning",
        "choices": [
            "Bias increase ; Variance increase",
            "Bias increase ; Variance decrease",
            "Bias decrease ; Variance increase",
            "Bias decrease ; Variance decrease"
        ],
        "answer": 1
    },
    {
        "instruction 1": "Identify the specific PyTorch functions used for generating random samples from the Gaussian distribution and the uniform distribution, ensuring to correctly set the mean and standard deviation for the Gaussian matrix and the range for the uniform matrix.",
        "instruction 2": "Combine the identified functions into a single code snippet, making sure to specify the dimensions for both the Gaussian and uniform matrices while clearly defining any necessary parameters or configurations in the PyTorch commands.",
        "question": "Which PyTorch 1.8 command(s) produce $10\\times 5$ Gaussian matrix with each entry i.i.d. sampled from $\\mathcal{N}(\\mu=5,\\sigma^2=16)$ and a $10\\times 10$ uniform matrix with each entry i.i.d. sampled from $U[-1,1)$?",
        "choices": [
            "\\texttt{5 + torch.randn(10,5) * 16} ; \\texttt{torch.rand(10,10,low=-1,high=1)}",
            "\\texttt{5 + torch.randn(10,5) * 16} ; \\texttt{(torch.rand(10,10) - 0.5) / 0.5}",
            "\\texttt{5 + torch.randn(10,5) * 4} ; \\texttt{2 * torch.rand(10,10) - 1}",
            "\\texttt{torch.normal(torch.ones(10,5)*5,torch.ones(5,5)*16)} ; \\texttt{2 * torch.rand(10,10) - 1}"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Analyze the properties of the ReLU and sigmoid activation functions in terms of their gradients for different ranges of input values, specifically focusing on the continuity and behavior of their gradients, and how these affect learning in neural networks.",
        "instruction 2": "Compare the implications of the gradient characteristics on optimization and learning dynamics in neural networks, discussing how the behavior of these gradients influences convergence and model performance.",
        "question": "Statement 1| The ReLU's gradient is zero for $x<0$, and the sigmoid gradient $\\sigma(x)(1-\\sigma(x))\\le \\frac{1}{4}$ for all $x$. Statement 2| The sigmoid has a continuous gradient and the ReLU has a discontinuous gradient.",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "instruction 1": "Identify the primary function and advantages of Batch Normalization in deep learning, specifically focusing on how it helps stabilize and accelerate training by normalizing the inputs to each layer.",
        "instruction 2": "Examine the implications of Batch Normalization on model performance, including its effects on gradient flow and its role in mitigating issues such as internal covariate shift.",
        "question": "Which is true about Batch Normalization?",
        "choices": [
            "After applying batch normalization, the layer’s activations will follow a standard Gaussian distribution.",
            "The bias parameter of affine layers becomes redundant if a batch normalization layer follows immediately afterward.",
            "The standard weight initialization must be changed when using Batch Normalization.",
            "Batch Normalization is equivalent to Layer Normalization for convolutional neural networks."
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Apply the chain rule to differentiate the objective function with respect to the variable w. This includes differentiating the terms involving the norm squared of the residuals (Xw - y) and the regularization term (norm squared of w).",
        "instruction 2": "Combine the results of the differentiation to express the gradient as a vector, ensuring to account for any constants in front of the terms in the original objective function and accurately compiling both parts into a single gradient expression.",
        "question": "Suppose we have the following objective function: $\\argmin_{w} \\frac{1}{2} \\norm{Xw-y}^2_2 + \\frac{1}{2}\\gamma \\norm{w}^2_2$ What is the gradient of $\\frac{1}{2} \\norm{Xw-y}^2_2 + \\frac{1}{2}\\lambda \\norm{w}^2_2$ with respect to $w$?",
        "choices": [
            "$\\nabla_w f(w) = (X^\\top X + \\lambda I)w - X^\\top y + \\lambda w$",
            "$\\nabla_w f(w) = X^\\top X w - X^\\top y + \\lambda$",
            "$\\nabla_w f(w) = X^\\top X w - X^\\top y + \\lambda w$",
            "$\\nabla_w f(w) = X^\\top X w - X^\\top y + (\\lambda+1) w$"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Review the definition and functionality of a convolution kernel in the context of image processing, particularly how it operates on input data to produce output by emphasizing certain features or patterns within the data.",
        "instruction 2": "Evaluate the various properties of convolution kernels, such as size, shape, and effects on data, to identify specific truths regarding their application in operations like blurring, sharpening, or edge detection.",
        "question": "Which of the following is true of a convolution kernel?",
        "choices": [
            "Convolving an image with $\\begin{bmatrix}1 & 0 & 0\\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$ would not change the image",
            "Convolving an image with $\\begin{bmatrix}0 & 0 & 0\\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$ would not change the image",
            "Convolving an image with $\\begin{bmatrix}1 & 1 & 1\\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}$ would not change the image",
            "Convolving an image with $\\begin{bmatrix}0 & 0 & 0\\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$ would not change the image"
        ],
        "subject": "machine_learning",
        "answer": 1
    },
    {
        "instruction 1": "Clarify the context or content for the options provided in the question to accurately assess which statement may be false, as the question lacks specific details on the statements or choices to evaluate.",
        "instruction 2": "Apply critical thinking and knowledge of the subject matter to systematically analyze each option, checking for factual inaccuracies or logical inconsistencies to determine which statement does not hold true.",
        "question": "Which of the following is false?",
        "choices": [
            "The following fully connected network without activation functions is linear: $g_3(g_2(g_1(x)))$, where $g_i(x) = W_i x$ and $W_i$ are matrices.",
            "Leaky ReLU $\\max\\{0.01x,x\\}$ is convex.",
            "A combination of ReLUs such as $ReLU(x) - ReLU(x-1)$ is convex.",
            "The loss $\\log \\sigma(x)= -\\log(1+e^{-x})$ is concave"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Clarify the context or content for the options provided in the question to accurately assess which statement may be false, as the question lacks specific details on the statements or choices to evaluate.",
        "instruction 2": "Apply critical thinking and knowledge of the subject matter to systematically analyze each option, checking for factual inaccuracies or logical inconsistencies to determine which statement does not hold true.",
        "question": "Which of the following is false?",
        "choices": [
            "The following fully connected network without activation functions is linear: $g_3(g_2(g_1(x)))$, where $g_i(x) = W_i x$ and $W_i$ are matrices.",
            "Leaky ReLU $\\max\\{0.01x,x\\}$ is convex.",
            "A combination of ReLUs such as $ReLU(x) - ReLU(x-1)$ is convex.",
            "The loss $\\log \\sigma(x)= -\\log(1+e^{-x})$ is concave"
        ],
        "subject": "machine_learning",
        "answer": 2
    },
    {
        "instruction 1": "Calculate the number of parameters for each layer in the network. For the first hidden layer, account for the weights connecting the input dimension (100) to the hidden layer (1000) and the biases for each of the 1000 neurons. For the second hidden layer, account for the weights connecting the first hidden layer (1000) to the second hidden layer (10) and the biases for each of the 10 neurons. Finally, calculate the output layer parameters, which involve weights connecting the second hidden layer (10) to the output (1) and the bias for the scalar output.",
        "instruction 2": "Sum the total parameters from all layers (i.e., first hidden layer, second hidden layer, and output layer) to find the overall number of parameters in the fully connected network, ensuring all connections and biases are included in the calculations.",
        "question": "We are training fully connected network with two hidden layers to predict housing prices. Inputs are $100$-dimensional, and have several features such as the number of square feet, the median family income, etc. The first hidden layer has $1000$ activations. The second hidden layer has $10$ activations. The output is a scalar representing the house price. Assuming a vanilla network with affine transformations and with no batch normalization and no learnable parameters in the activation function, how many parameters does this network have?",
        "choices": [
            "111021",
            "110010",
            "111110",
            "110011"
        ],
        "subject": "machine_learning",
        "answer": 0
    },
    {
        "question": "Statement 1| The derivative of the sigmoid $\\sigma(x)=(1+e^{-x})^{-1}$ with respect to $x$ is equal to $\\text{Var}(B)$ where $B\\sim \\text{Bern}(\\sigma(x))$ is a Bernoulli random variable. Statement 2| Setting the bias parameters in each layer of neural network to 0 changes the bias-variance trade-off such that the model's variance increases and the model's bias decreases",
        "subject": "machine_learning",
        "choices": [
            "True, True",
            "False, False",
            "True, False",
            "False, True"
        ],
        "answer": 2
    }
]